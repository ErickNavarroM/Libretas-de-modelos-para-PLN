{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>LSTM básico: Haciendo N-gramas</h1>\n",
    "\n",
    "\n",
    "<p> Julio Waissman Vilanova </p>\n",
    "\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mcd-unison/pln/blob/main/labs/RNN/deep-ngram.ipynb\"><img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\"  width=\"30\" /> Ejecuta en Colab</a>\n",
    "\n",
    "Tomado parcialmente y adaptado de [el repositorio de github](https://github.com/shaundsouza/lstm-textual-ngrams) del trabajo [*LSTM Neural Network for Textual Ngrams* (D'Souza, 2018)](https://www.preprints.org/manuscript/201811.0579/v1)\n",
    "\n",
    "\n",
    "\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando datos de alguna obra \n",
    "\n",
    "Vamos a descargar un *Corpus*, y pues vamos a usar algo muy famoso, como  puede ser la obra de Shakespiare en inglés, o *El Quijote* en español. Empecemos por *El Quijote*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      " 30 2173k   30  661k    0     0   279k      0  0:00:07  0:00:02  0:00:05  280k\n",
      "100 2173k  100 2173k    0     0   800k      0  0:00:02  0:00:02 --:--:--  801k\n"
     ]
    }
   ],
   "source": [
    "!curl -o quijote.txt https://www.gutenberg.org/cache/epub/2000/pg2000.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y ahora vamos a descargar el corpus por lineas de texto, eliminando cabecera y final de Gutemberg, quitando espacios, moviendo todas las letras a minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = \"quijote.txt\"\n",
    "corpus = []\n",
    "with open(archivo, 'r', encoding='utf8') as fp:\n",
    "    guarda = False\n",
    "    for line in fp.readlines():\n",
    "      if \"*** END OF THE PROJECT GUTENBERG\" in line:\n",
    "        break\n",
    "      if guarda and len(line) > 2:\n",
    "        corpus.append(line.strip())\n",
    "      if \"*** START OF THE PROJECT GUTENBERG\" in line:\n",
    "        guarda = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos como de ve el corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio del texto: \n",
      "['El ingenioso hidalgo don Quijote de la Mancha', 'por Miguel de Cervantes Saavedra', 'El ingenioso hidalgo don Quijote de la Mancha', '', 'Tasa', '', 'Testimonio de las erratas', '', 'El Rey', '']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Inicio del texto: \\n{corpus[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin del texto: \n",
      "['para hacer burla de tantas como hicieron tantos andantes caballeros, bastan', 'las dos que él hizo, tan a gusto y beneplácito de las gentes a cuya noticia', \"llegaron, así en éstos como en los estraños reinos''. Y con esto cumplirás\", 'con tu cristiana profesión, aconsejando bien a quien mal te quiere, y yo', 'quedaré satisfecho y ufano de haber sido el primero que gozó el fruto de', 'sus escritos enteramente, como deseaba, pues no ha sido otro mi deseo que', 'poner en aborrecimiento de los hombres las fingidas y disparatadas', 'historias de los libros de caballerías, que, por las de mi verdadero don', 'Quijote, van ya tropezando, y han de caer del todo, sin duda alguna. Vale.', 'Fin']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Fin del texto: \\n{corpus[-10:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "31941\n"
     ]
    }
   ],
   "source": [
    "print(type(corpus))\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesando la información \n",
    "\n",
    "Para el preprocesamiento, vamos a convertir a tokens cada linea de texto y vamos a agregar pads al mas largo de los textos que se encuentran en el documento (antes de cada salto de linea). Vamos a utilizar la capa de `TextVectorization` que ofrece `Keras`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 10_000\n",
    "\n",
    "indizador = layers.TextVectorization(max_tokens=max_tokens)\n",
    "indizador.adapt(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo que tenemos un nuevo vocabulario que podemos ver en orden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenemos un vocabulario de 10000 tokens\n",
      "Y aquí están las primeras 20 palabras:\n",
      "['', '[UNK]', 'que', 'de', 'y', 'la', 'a', 'el', 'en', 'no', 'los', 'se', 'con', 'por', 'lo', 'las', 'le', 'su', '—', 'don']\n"
     ]
    }
   ],
   "source": [
    "vocab = indizador.get_vocabulary()\n",
    "print(f\"Tenemos un vocabulario de {len(vocab)} tokens\")\n",
    "print(f\"Y aquí están las primeras 20 palabras:\\n{vocab[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora vamos a convertir el *corpus* en tensores de tokens simplemente como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las entradas en un tensor con un shape de:\n",
      "(31941, 20)\n"
     ]
    }
   ],
   "source": [
    "entradas = indizador(corpus)\n",
    "\n",
    "print(f\"Las entradas en un tensor con un shape de:\\n{entradas.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver tenemos una serie de tokens por linea (o muestra) que se generó del *corpus*. Ahora vamos poniendo la secuencia de salida. \n",
    "\n",
    "Como lo que queremos es estimar la próxima palabra, pues no queda mas que usar los mismos tokens de entrada, pero con un corrimiento hacia la izquierda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada 1,000: \n",
      "[ 574  901 2131    3    1    8 6373    1    8 5825    4    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "Salida 1,000: \n",
      "[ 901 2131    3    1    8 6373    1    8 5825    4    0    0    0    0\n",
      "    0    0    0    0    0  574]\n"
     ]
    }
   ],
   "source": [
    "salidas = tf.roll(entradas, shift=-1, axis=1)\n",
    "\n",
    "print(f\"Entrada 1,000: \\n{entradas[1_000]}\")\n",
    "print(f\"Salida 1,000: \\n{salidas[1_000]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, las salidas son iguales a las entradas pero con un adelanto, así ya podemos especificar nuestro modelo de N-gramas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo y entrenamiento\n",
    "\n",
    "El modelo es muy sencillo y podemos discutirlo mucho y modificarlo para probar nuevas cosas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
