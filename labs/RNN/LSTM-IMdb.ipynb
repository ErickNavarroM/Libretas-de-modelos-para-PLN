{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>LSTM con Keras, un flujo básico pero completo</h1>\n",
    "\n",
    "\n",
    "<p> Julio Waissman Vilanova </p>\n",
    "\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mcd-unison/pln/blob/main/labs/RNN/LSTM-IMdb.ipynb\"><img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\"  width=\"30\" /> Ejecuta en Colab</a>\n",
    "\n",
    "<p>\n",
    "Tomado parcialmente y adaptado de varias libretas de la documentación de Keras\n",
    "</p>\n",
    "\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obteniendo datos\n",
    "\n",
    "Vamos a recuperar la base de datos globera de IMdb que se usa para probar casi todos los modelos. Vamos a recuperar los adatos de \n",
    "\n",
    "``https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  3 80.2M    3 2592k    0     0  1278k      0  0:01:04  0:00:02  0:01:02 1281k\n",
      "  9 80.2M    9 8096k    0     0  2675k      0  0:00:30  0:00:03  0:00:27 2679k\n",
      " 13 80.2M   13 10.9M    0     0  2751k      0  0:00:29  0:00:04  0:00:25 2754k\n",
      " 16 80.2M   16 13.6M    0     0  2780k      0  0:00:29  0:00:05  0:00:24 2787k\n",
      " 21 80.2M   21 17.3M    0     0  2960k      0  0:00:27  0:00:06  0:00:21 3626k\n",
      " 25 80.2M   25 20.7M    0     0  3024k      0  0:00:27  0:00:07  0:00:20 3734k\n",
      " 29 80.2M   29 23.8M    0     0  3043k      0  0:00:26  0:00:08  0:00:18 3267k\n",
      " 33 80.2M   33 27.0M    0     0  3061k      0  0:00:26  0:00:09  0:00:17 3312k\n",
      " 37 80.2M   37 30.3M    0     0  3093k      0  0:00:26  0:00:10  0:00:16 3404k\n",
      " 42 80.2M   42 34.2M    0     0  3185k      0  0:00:25  0:00:11  0:00:14 3457k\n",
      " 48 80.2M   48 39.0M    0     0  3328k      0  0:00:24  0:00:12  0:00:12 3754k\n",
      " 54 80.2M   54 43.4M    0     0  3419k      0  0:00:24  0:00:13  0:00:11 4023k\n",
      " 60 80.2M   60 48.5M    0     0  3552k      0  0:00:23  0:00:14  0:00:09 4449k\n",
      " 67 80.2M   67 54.0M    0     0  3681k      0  0:00:22  0:00:15  0:00:07 4867k\n",
      " 73 80.2M   73 59.2M    0     0  3788k      0  0:00:21  0:00:16  0:00:05 5117k\n",
      " 81 80.2M   81 65.2M    0     0  3928k      0  0:00:20  0:00:17  0:00:03 5376k\n",
      " 89 80.2M   89 71.6M    0     0  4074k      0  0:00:20  0:00:18  0:00:02 5781k\n",
      " 97 80.2M   97 78.5M    0     0  4231k      0  0:00:19  0:00:19 --:--:-- 6133k\n",
      "100 80.2M  100 80.2M    0     0  4274k      0  0:00:19  0:00:19 --:--:-- 6397k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " y vamos a investigas la estructura y lo que hay..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls aclImdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls aclImdb/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls aclImdb/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat aclImdb/train/pos/6248_7.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo nos interesan las evaluaciones positivas y negativas (para hacer una simple clasificación binaria y simplificar la aplicación), por lo que vamos a borrar el folder `unsup`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora si, vamos a usar las librerías de `Keras` para leer los datos usando [`keras.utils.text_dataset_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/text_dataset_from_directory).\n",
    "\n",
    "En este momento es donde tenemos que determinar el tamaño de los lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32         # Tamaño de los minibatches\n",
    "\n",
    "raw_train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    ")\n",
    "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    ")\n",
    "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Numero de batches en raw_train_ds: {raw_train_ds.cardinality()}\")\n",
    "print(f\"Numero de batches en raw_val_ds: {raw_val_ds.cardinality()}\")\n",
    "print(f\"Numero de batches en raw_test_ds: {raw_test_ds.cardinality()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante revisar los datos crudos para tener una idea de como se recuperaron y cual es la forma que tienen.\n",
    "\n",
    "Esto lo podemos hacer tomando algunos datos de cada batch e imprimiendolos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    for i in range(5):\n",
    "        print(textwrap.fill(text_batch.numpy()[i], 80, subsequent_indent='> '))\n",
    "        print(\"\\ntarget =\", label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando los datos\n",
    "\n",
    "Vamos ahora a convertir cada string de datos en una serie de índices numéricos, los cuales puedan entrar en \n",
    "un modelo neuronal. Para esto, vamos a generar índices a partir de las palabras existentesd en el texto.\n",
    "\n",
    "Este métdo puede ser no el mejor, ya que el vocabulario se fija en relación al vocabulario encontrado en el\n",
    "conjunto de aprendizaje. Más adelante veremos mñetodos más sofisticados para hacer la indezación, o como\n",
    "usar un vocabulario indexado ya preestablecido.\n",
    "\n",
    "Por el momento vamos primero a especificar el proceso de limpieza de texto (preprocesamiento) el cual será muy sencillo para este ejemplo y consiste en:\n",
    "\n",
    "1. Convertir a minúsculas todas las letras\n",
    "2. Eliminar los saltos de linea en formato *html* ( `<br /> `) \n",
    "3. Eliminar los signos de puntuación\n",
    "\n",
    "Igualmente, vamos a generar los minibatches con secuencias de `sequence_length` palabras. Esto es, si es insuficiente, se trunca el texto y si es \n",
    "demasiado, se completa el texto con 0's. De esa manera, todos los modelos aprenden con secuencias del mismo tamaño.\n",
    "\n",
    "Se utilizan hasta `max_features` tokens diferentes. De haber más, estos se eliminan en función de su frecuencia.\n",
    "\n",
    "Para esto vamos a utilizar la capa de `Keras` de [`layers.TextVectorization`](https://keras.io/api/layers/preprocessing_layers/text/text_vectorization/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model constants.\n",
    "max_features = 20000\n",
    "sequence_length = 500\n",
    "\n",
    "# Preprocesamiento\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Capa de vectorización (encontrar los índices por palabra)\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "# Now that the vectorize_layer has been created, call `adapt` on a text-only\n",
    "# dataset to create the vocabulary. You don't have to batch, but for very large\n",
    "# datasets this means you're not keeping spare copies of the dataset in memory.\n",
    "\n",
    "# Let's make a text-only dataset (no labels):\n",
    "text_ds = raw_train_ds.map(lambda x, y: x)\n",
    "\n",
    "# Let's call `adapt`:\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data.\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do async prefetching / buffering of the data for best performance on GPU.\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Donde se guardan los datos de entrenamiento\")\n",
    "print(\"train_ds.cardinality() = \", train_ds.cardinality())\n",
    "\n",
    "ejemplo = train_ds.take(1)\n",
    "\n",
    "print(\"\\nY un minibatch se representa de esta manera: \\n\")\n",
    "print(ejemplo.get_single_element())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo basado en LSTM multicapa\n",
    "\n",
    "Vamos a hacer un modelo multicapa, el cual seguramente requerirá de ajustes de su parte.\n",
    "\n",
    "Vamos a utilizar la forma funcional de definir un modelo neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = 128               # Embedding size\n",
    "unidades = 128          # Hidden units per layer\n",
    "\n",
    "\n",
    "# Entrada en indices\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# Capa de embeddings\n",
    "x = layers.Embedding(max_features, emb)(inputs)\n",
    "\n",
    "# Dos capas de LSTMs\n",
    "x = layers.LSTM(unidades, return_sequences=True)(x)\n",
    "x = layers.LSTM(unidades)(x)\n",
    "\n",
    "# Salida\n",
    "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs, predictions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilamos y ponemos a aprender el modelo (usando BPTT en forma automñatica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    \"adam\",\n",
    "    \"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y probamos con los datos de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora vamos a probar con bi-LSTM, haciendo un poco más complicado (aunque no mucho) el código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = 128\n",
    "unidades = 128\n",
    "\n",
    "# Input \n",
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "# Capa de embeddings\n",
    "x = layers.Embedding(max_features, emb)(inputs)\n",
    "\n",
    "# bi-LSTMs\n",
    "x = layers.Bidirectional(\n",
    "    layers.LSTM(unidades, return_sequences=True)\n",
    ")(x)\n",
    "x = layers.Bidirectional(\n",
    "    layers.LSTM(unidades)\n",
    ")(x)\n",
    "\n",
    "# Vanilla hidden layer:\n",
    "x = layers.Dense(unidades, activation=\"relu\")(x)\n",
    "\n",
    "# Salida\n",
    "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "model_bi = keras.Model(inputs, predictions)\n",
    "model_bi.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bi.compile(\n",
    "    \"adam\",\n",
    "    \"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model_bi.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bi.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo por convolucionales de 1 dimensión\n",
    "\n",
    "Este modelo viene como modelo de base en Keras, y es un buen inicio para ver como usar convolucionales como modelos para PLN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = 128\n",
    "unidades = 128\n",
    "ventana = 7\n",
    "drop= 0.5\n",
    "\n",
    "# Entrada\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# Capa de embeddings\n",
    "x = layers.Embedding(max_features, emb)(inputs)\n",
    "x = layers.Dropout(drop)(x)\n",
    "\n",
    "# Conv1D + global max pooling\n",
    "x = layers.Conv1D(\n",
    "    unidades, \n",
    "    ventana, \n",
    "    padding=\"valid\", \n",
    "    activation=\"relu\", \n",
    "    strides=3\n",
    ")(x)\n",
    "x = layers.Conv1D(\n",
    "    unidades, \n",
    "    ventana, \n",
    "    padding=\"valid\", \n",
    "    activation=\"relu\", \n",
    "    strides=3\n",
    ")(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# Vanilla hidden layer:\n",
    "x = layers.Dense(unidades, activation=\"relu\")(x)\n",
    "x = layers.Dropout(drop)(x)\n",
    "\n",
    "# Salida\n",
    "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "model_conv1d = tf.keras.Model(inputs, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "model_conv1d.compile(\n",
    "    loss=\"binary_crossentropy\", \n",
    "    optimizer=\"adam\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds, \n",
    "    epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv1d.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo para producción\n",
    "\n",
    "Si ya tenemos nuestro modelo funcionando, y nos gusta, y queremos dejarlo en un formato que permita aplicarlo a los datos en crudo, es necesario empaquetar todo nuestro procedimiento en un solo procedimiento de principio a fin. \n",
    "\n",
    "Agregamos aqui el truco para empaqetar todo, cuando ya no se espera reentrenar el modelo (al menos no en el corto plazo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_seleccionado = model_bi\n",
    "\n",
    "# A string input\n",
    "inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
    "\n",
    "# Turn strings into vocab indices\n",
    "indices = vectorize_layer(inputs)\n",
    "\n",
    "# Turn vocab indices into predictions\n",
    "outputs = modelo_seleccionado(indices)\n",
    "\n",
    "# Our end to end model\n",
    "end_to_end_model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "end_to_end_model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "end_to_end_model.save('nombre_codigo.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_to_end_model = keras.saving.load_model(\"nombre_codigo.keras\")\n",
    "\n",
    "# Test it with `raw_test_ds`, which yields raw strings\n",
    "end_to_end_model.evaluate(raw_test_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
